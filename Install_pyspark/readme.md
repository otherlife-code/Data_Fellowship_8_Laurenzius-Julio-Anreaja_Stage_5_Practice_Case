# PySpark Installation

1. Presequisites
Make sure you have installed the following :
- Java
- Anaconda
- Python
- pip

2. Install Java Development kit (JDK)
Install JDK from this [link][https://www.oracle.com/java/technologies/downloads/]

3. Create new user variable with name "JAVA_HOME" and set it's path to yout JDK installation directory

4. Create new user variable and name it with "PATH" and set it's path to your JDK/bin directory

5. Download spark installation files from [spark homepage][https://spark.apache.org/downloads.html]

6. Create 2 variables and name it with "hadoop_home" and "spark_home" and set it to your spark directory

7. Run "pyspark" in your coomand prompt to check if the installation is completed
